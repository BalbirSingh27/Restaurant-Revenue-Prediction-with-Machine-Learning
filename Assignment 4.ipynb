{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fabcb488",
   "metadata": {},
   "source": [
    "# Restaurant Revenue Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff73e57f",
   "metadata": {},
   "source": [
    "# Importing Libraries, Loading Data, EDA, Data Preprocessing, Feature Selection and Normalization, Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ccaf7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (137, 43)\n",
      "Test data shape: (100000, 42)\n",
      "Train columns: Index(['Id', 'Open Date', 'City', 'City Group', 'Type', 'P1', 'P2', 'P3', 'P4',\n",
      "       'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 'P12', 'P13', 'P14', 'P15',\n",
      "       'P16', 'P17', 'P18', 'P19', 'P20', 'P21', 'P22', 'P23', 'P24', 'P25',\n",
      "       'P26', 'P27', 'P28', 'P29', 'P30', 'P31', 'P32', 'P33', 'P34', 'P35',\n",
      "       'P36', 'P37', 'revenue'],\n",
      "      dtype='object')\n",
      "Test columns: Index(['Id', 'Open Date', 'City', 'City Group', 'Type', 'P1', 'P2', 'P3', 'P4',\n",
      "       'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 'P12', 'P13', 'P14', 'P15',\n",
      "       'P16', 'P17', 'P18', 'P19', 'P20', 'P21', 'P22', 'P23', 'P24', 'P25',\n",
      "       'P26', 'P27', 'P28', 'P29', 'P30', 'P31', 'P32', 'P33', 'P34', 'P35',\n",
      "       'P36', 'P37'],\n",
      "      dtype='object')\n",
      "Missing values in train data:\n",
      " Id            0\n",
      "Open Date     0\n",
      "City          0\n",
      "City Group    0\n",
      "Type          0\n",
      "P1            0\n",
      "P2            0\n",
      "P3            0\n",
      "P4            0\n",
      "P5            0\n",
      "P6            0\n",
      "P7            0\n",
      "P8            0\n",
      "P9            0\n",
      "P10           0\n",
      "P11           0\n",
      "P12           0\n",
      "P13           0\n",
      "P14           0\n",
      "P15           0\n",
      "P16           0\n",
      "P17           0\n",
      "P18           0\n",
      "P19           0\n",
      "P20           0\n",
      "P21           0\n",
      "P22           0\n",
      "P23           0\n",
      "P24           0\n",
      "P25           0\n",
      "P26           0\n",
      "P27           0\n",
      "P28           0\n",
      "P29           0\n",
      "P30           0\n",
      "P31           0\n",
      "P32           0\n",
      "P33           0\n",
      "P34           0\n",
      "P35           0\n",
      "P36           0\n",
      "P37           0\n",
      "revenue       0\n",
      "dtype: int64\n",
      "Missing values in test data:\n",
      " Id            0\n",
      "Open Date     0\n",
      "City          0\n",
      "City Group    0\n",
      "Type          0\n",
      "P1            0\n",
      "P2            0\n",
      "P3            0\n",
      "P4            0\n",
      "P5            0\n",
      "P6            0\n",
      "P7            0\n",
      "P8            0\n",
      "P9            0\n",
      "P10           0\n",
      "P11           0\n",
      "P12           0\n",
      "P13           0\n",
      "P14           0\n",
      "P15           0\n",
      "P16           0\n",
      "P17           0\n",
      "P18           0\n",
      "P19           0\n",
      "P20           0\n",
      "P21           0\n",
      "P22           0\n",
      "P23           0\n",
      "P24           0\n",
      "P25           0\n",
      "P26           0\n",
      "P27           0\n",
      "P28           0\n",
      "P29           0\n",
      "P30           0\n",
      "P31           0\n",
      "P32           0\n",
      "P33           0\n",
      "P34           0\n",
      "P35           0\n",
      "P36           0\n",
      "P37           0\n",
      "dtype: int64\n",
      "Train data summary:\n",
      "                Id          P1          P2          P3          P4          P5  \\\n",
      "count  137.000000  137.000000  137.000000  137.000000  137.000000  137.000000   \n",
      "mean    68.000000    4.014599    4.408759    4.317518    4.372263    2.007299   \n",
      "std     39.692569    2.910391    1.514900    1.032337    1.016462    1.209620   \n",
      "min      0.000000    1.000000    1.000000    0.000000    3.000000    1.000000   \n",
      "25%     34.000000    2.000000    4.000000    4.000000    4.000000    1.000000   \n",
      "50%     68.000000    3.000000    5.000000    4.000000    4.000000    2.000000   \n",
      "75%    102.000000    4.000000    5.000000    5.000000    5.000000    2.000000   \n",
      "max    136.000000   12.000000    7.500000    7.500000    7.500000    8.000000   \n",
      "\n",
      "               P6          P7          P8          P9  ...         P29  \\\n",
      "count  137.000000  137.000000  137.000000  137.000000  ...  137.000000   \n",
      "mean     3.357664    5.423358    5.153285    5.445255  ...    3.135036   \n",
      "std      2.134235    2.296809    1.858567    1.834793  ...    1.680887   \n",
      "min      1.000000    1.000000    1.000000    4.000000  ...    0.000000   \n",
      "25%      2.000000    5.000000    4.000000    4.000000  ...    2.500000   \n",
      "50%      3.000000    5.000000    5.000000    5.000000  ...    3.000000   \n",
      "75%      4.000000    5.000000    5.000000    5.000000  ...    3.000000   \n",
      "max     10.000000   10.000000   10.000000   10.000000  ...    7.500000   \n",
      "\n",
      "              P30         P31         P32         P33         P34         P35  \\\n",
      "count  137.000000  137.000000  137.000000  137.000000  137.000000  137.000000   \n",
      "mean     2.729927    1.941606    2.525547    1.138686    2.489051    2.029197   \n",
      "std      5.536647    3.512093    5.230117    1.698540    5.165093    3.436272   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "75%      4.000000    3.000000    3.000000    2.000000    3.000000    4.000000   \n",
      "max     25.000000   15.000000   25.000000    6.000000   24.000000   15.000000   \n",
      "\n",
      "              P36         P37       revenue  \n",
      "count  137.000000  137.000000  1.370000e+02  \n",
      "mean     2.211679    1.116788  4.453533e+06  \n",
      "std      4.168211    1.790768  2.576072e+06  \n",
      "min      0.000000    0.000000  1.149870e+06  \n",
      "25%      0.000000    0.000000  2.999068e+06  \n",
      "50%      0.000000    0.000000  3.939804e+06  \n",
      "75%      3.000000    2.000000  5.166635e+06  \n",
      "max     20.000000    8.000000  1.969694e+07  \n",
      "\n",
      "[8 rows x 39 columns]\n",
      "Test data summary:\n",
      "                   Id             P1             P2             P3  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean    49999.500000       4.088030       4.428085       4.215325   \n",
      "std     28867.657797       2.812963       1.428865       0.842161   \n",
      "min         0.000000       1.000000       1.000000       0.000000   \n",
      "25%     24999.750000       2.000000       3.750000       4.000000   \n",
      "50%     49999.500000       3.000000       5.000000       4.000000   \n",
      "75%     74999.250000       4.000000       5.000000       4.000000   \n",
      "max     99999.000000      15.000000       7.500000       6.000000   \n",
      "\n",
      "                  P4             P5             P6            P7  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.00000   \n",
      "mean        4.396025       1.989590       2.881900       5.30051   \n",
      "std         1.035827       1.065314       1.531429       2.17858   \n",
      "min         2.000000       1.000000       1.000000       1.00000   \n",
      "25%         4.000000       1.000000       2.000000       5.00000   \n",
      "50%         4.000000       2.000000       2.000000       5.00000   \n",
      "75%         5.000000       2.000000       4.000000       5.00000   \n",
      "max         7.500000       6.000000      10.000000      10.00000   \n",
      "\n",
      "                 P8             P9  ...            P28            P29  \\\n",
      "count  100000.00000  100000.000000  ...  100000.000000  100000.000000   \n",
      "mean        4.93100       5.251380  ...       3.233785       3.084000   \n",
      "std         1.71849       1.702632  ...       2.136694       1.783927   \n",
      "min         1.00000       4.000000  ...       1.000000       0.000000   \n",
      "25%         4.00000       4.000000  ...       2.000000       2.000000   \n",
      "50%         5.00000       5.000000  ...       3.000000       3.000000   \n",
      "75%         5.00000       5.000000  ...       4.000000       3.000000   \n",
      "max        10.00000      10.000000  ...      12.500000      10.000000   \n",
      "\n",
      "                 P30            P31            P32            P33  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean        2.083300       1.193330       1.942640       0.987430   \n",
      "std         4.309479       2.307944       3.971298       1.534808   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       0.000000   \n",
      "50%         0.000000       0.000000       0.000000       0.000000   \n",
      "75%         3.000000       1.000000       3.000000       2.000000   \n",
      "max        25.000000      15.000000      25.000000       6.000000   \n",
      "\n",
      "                 P34            P35            P36            P37  \n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000  \n",
      "mean        2.108670       1.832830       1.968890       0.973500  \n",
      "std         4.685414       3.228769       3.805773       1.677267  \n",
      "min         0.000000       0.000000       0.000000       0.000000  \n",
      "25%         0.000000       0.000000       0.000000       0.000000  \n",
      "50%         0.000000       0.000000       0.000000       0.000000  \n",
      "75%         3.000000       4.000000       3.000000       2.000000  \n",
      "max        30.000000      15.000000      20.000000       8.000000  \n",
      "\n",
      "[8 rows x 38 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed training data shape after feature selection: (137, 10)\n",
      "Transformed test data shape after feature selection: (100000, 10)\n",
      "Random Forest RMSE: 3266855.7595323743\n",
      "Decision Tree RMSE: 3506768.7904327866\n",
      "SVR RMSE: 3607694.057201298\n",
      "MLP RMSE: 5903166.119717886\n",
      "SGD Regressor RMSE: 3721329.2507195408\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Importing required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Step 2: Loading the data\n",
    "\n",
    "train = pd.read_csv(r'C:\\Users\\ayush\\Downloads\\restaurant-revenue-prediction\\train1.csv')\n",
    "test = pd.read_csv(r'C:\\Users\\ayush\\Downloads\\restaurant-revenue-prediction\\test1.csv')\n",
    "\n",
    "# Checking the columns and first few rows to ensure the data is loaded correctly\n",
    "print(\"Train data shape:\", train.shape)\n",
    "print(\"Test data shape:\", test.shape)\n",
    "\n",
    "# Check column names\n",
    "print(\"Train columns:\", train.columns)\n",
    "print(\"Test columns:\", test.columns)\n",
    "\n",
    "# Step 3: Perform EDA\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in train data:\\n\", train.isnull().sum())\n",
    "print(\"Missing values in test data:\\n\", test.isnull().sum())\n",
    "\n",
    "# Summary statistics\n",
    "print(\"Train data summary:\\n\", train.describe())\n",
    "print(\"Test data summary:\\n\", test.describe())\n",
    "\n",
    "# Visualize distributions (if required)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train['revenue'], kde=True)\n",
    "plt.title('Distribution of Revenue in Training Data')\n",
    "plt.show()\n",
    "\n",
    "# Fill missing values for numerical columns with the mean\n",
    "train.fillna(train.mean(), inplace=True)  \n",
    "test.fillna(test.mean(), inplace=True)\n",
    "\n",
    "# Drop unnecessary columns (e.g., 'Id' and 'Open Date') if they exist\n",
    "columns_to_drop = ['Id', 'Open Date']\n",
    "train.drop(columns=[col for col in columns_to_drop if col in train.columns], inplace=True)\n",
    "test.drop(columns=[col for col in columns_to_drop if col in test.columns], inplace=True)\n",
    "\n",
    "# Separate features (X) and target variable (y) in the training data\n",
    "X_train = train.drop('revenue', axis=1)  # Drop 'revenue' from features\n",
    "y_train = train['revenue']  # Target variable\n",
    "\n",
    "# Test data should not include 'revenue' (as it is not part of the test set)\n",
    "X_test = test  # Features in test set\n",
    "\n",
    "# Step 4: Preprocess, Feature Selection, and Normalization\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = ['City', 'City Group', 'Type']  # These are the columns with string values\n",
    "\n",
    "# Separate the categorical and numerical columns\n",
    "numerical_cols = [col for col in X_train.columns if col not in categorical_cols]\n",
    "\n",
    "# Create the preprocessor with OneHotEncoding for categorical columns and StandardScaler for numerical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', StandardScaler(), numerical_cols),  # StandardScaler for numerical columns\n",
    "                  ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)  # OneHotEncoder for categorical\n",
    "    ])\n",
    "\n",
    "# Apply the preprocessor to the data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Train a Random Forest Regressor to obtain feature importances\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_processed, y_train)  # Fit the Random Forest model to the processed data\n",
    "\n",
    "# Use SelectFromModel to select important features based on the feature importances from the Random Forest\n",
    "selector = SelectFromModel(rf, threshold=\"mean\", max_features=10, prefit=True)  # Set prefit=True\n",
    "\n",
    "# Apply the feature selection to both train and test sets\n",
    "X_train_selected = selector.transform(X_train_processed)  # Transform training data to select important features\n",
    "X_test_selected = selector.transform(X_test_processed)  # Transform test data to select important features\n",
    "\n",
    "# Check the shape after feature selection\n",
    "print(f\"Transformed training data shape after feature selection: {X_train_selected.shape}\")\n",
    "print(f\"Transformed test data shape after feature selection: {X_test_selected.shape}\")\n",
    "\n",
    "# Step 5: Split the Data into Training and Testing Sets\n",
    "\n",
    "# Use 80% for training and 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_selected, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Normalize the Data (Optional but often improves performance for certain models)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit to X_train and transform\n",
    "X_test_scaled = scaler.transform(X_test)  # Only transform X_test\n",
    "\n",
    "# Step 7: Initialize Models\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "    'SVR': SVR(),\n",
    "    'MLP': MLPRegressor(random_state=42),\n",
    "    'SGD Regressor': SGDRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Step 8: Train and Evaluate Models, including RMSE Calculation\n",
    "for name, model in models.items():\n",
    "    # Fit the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate RMSE (Root Mean Squared Error)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    # Print the RMSE for each model\n",
    "    print(f'{name} RMSE: {rmse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a48f47",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning of Regressor Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02167b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best hyperparameters for Random Forest:  {'n_estimators': 300, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_depth': 10, 'bootstrap': True}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best hyperparameters for Decision Tree:  {'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 20}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best hyperparameters for SVR:  {'kernel': 'linear', 'epsilon': 0.1, 'C': 100}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best hyperparameters for MLP:  {'solver': 'sgd', 'learning_rate': 'adaptive', 'hidden_layer_sizes': (150,), 'activation': 'tanh'}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best hyperparameters for SGD Regressor:  {'penalty': 'l2', 'max_iter': 1500, 'alpha': 0.001}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# 1. Random Forest Hyperparameter Tuning\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "rf_random_search = RandomizedSearchCV(RandomForestRegressor(random_state=42), param_distributions=rf_param_grid, \n",
    "                                      n_iter=10, cv=5, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "rf_random_search.fit(X_train_scaled, y_train)\n",
    "print(\"Best hyperparameters for Random Forest: \", rf_random_search.best_params_)\n",
    "\n",
    "# 2. Decision Tree Hyperparameter Tuning\n",
    "dt_param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "dt_random_search = RandomizedSearchCV(DecisionTreeRegressor(random_state=42), param_distributions=dt_param_grid, \n",
    "                                      n_iter=10, cv=5, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "dt_random_search.fit(X_train_scaled, y_train)\n",
    "print(\"Best hyperparameters for Decision Tree: \", dt_random_search.best_params_)\n",
    "\n",
    "# 3. Support Vector Regressor (SVR) Hyperparameter Tuning\n",
    "svr_param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'epsilon': [0.01, 0.1, 0.2],\n",
    "    'kernel': ['linear', 'poly', 'rbf']\n",
    "}\n",
    "\n",
    "svr_random_search = RandomizedSearchCV(SVR(), param_distributions=svr_param_grid, \n",
    "                                       n_iter=10, cv=5, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "svr_random_search.fit(X_train_scaled, y_train)\n",
    "print(\"Best hyperparameters for SVR: \", svr_random_search.best_params_)\n",
    "\n",
    "# 4. Multi-Layer Perceptron (MLP) Hyperparameter Tuning\n",
    "mlp_param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (150,)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive']\n",
    "}\n",
    "\n",
    "mlp_random_search = RandomizedSearchCV(MLPRegressor(random_state=42), param_distributions=mlp_param_grid, \n",
    "                                       n_iter=10, cv=5, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "mlp_random_search.fit(X_train_scaled, y_train)\n",
    "print(\"Best hyperparameters for MLP: \", mlp_random_search.best_params_)\n",
    "\n",
    "# 5. SGD Regressor Hyperparameter Tuning\n",
    "sgd_param_grid = {\n",
    "    'penalty': ['none', 'l2', 'l1'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'max_iter': [1000, 1500, 2000]\n",
    "}\n",
    "\n",
    "sgd_random_search = RandomizedSearchCV(SGDRegressor(random_state=42), param_distributions=sgd_param_grid, \n",
    "                                       n_iter=10, cv=5, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "sgd_random_search.fit(X_train_scaled, y_train)\n",
    "print(\"Best hyperparameters for SGD Regressor: \", sgd_random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "995a883b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - RMSE: 3329556.2679, R²: 0.0938\n",
      "Decision Tree - RMSE: 3333452.1706, R²: 0.0917\n",
      "SVR - RMSE: 3612264.4346, R²: -0.0666\n",
      "MLP Regressor - RMSE: 3577744.1894, R²: -0.0463\n",
      "SGD Regressor - RMSE: 3720325.8441, R²: -0.1313\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Evaluating Random Forest\n",
    "rf_best_model = rf_random_search.best_estimator_\n",
    "y_pred_rf = rf_best_model.predict(X_test_scaled)\n",
    "\n",
    "# RMSE for Random Forest\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Random Forest - RMSE: {rmse_rf:.4f}, R²: {r2_rf:.4f}\")\n",
    "\n",
    "# Evaluating Decision Tree\n",
    "dt_best_model = dt_random_search.best_estimator_\n",
    "y_pred_dt = dt_best_model.predict(X_test_scaled)\n",
    "\n",
    "# RMSE for Decision Tree\n",
    "rmse_dt = np.sqrt(mean_squared_error(y_test, y_pred_dt))\n",
    "r2_dt = r2_score(y_test, y_pred_dt)\n",
    "\n",
    "print(f\"Decision Tree - RMSE: {rmse_dt:.4f}, R²: {r2_dt:.4f}\")\n",
    "\n",
    "# Evaluating SVR\n",
    "svr_best_model = svr_random_search.best_estimator_\n",
    "y_pred_svr = svr_best_model.predict(X_test_scaled)\n",
    "\n",
    "# RMSE for SVR\n",
    "rmse_svr = np.sqrt(mean_squared_error(y_test, y_pred_svr))\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "\n",
    "print(f\"SVR - RMSE: {rmse_svr:.4f}, R²: {r2_svr:.4f}\")\n",
    "\n",
    "# Evaluating MLP Regressor\n",
    "mlp_best_model = mlp_random_search.best_estimator_\n",
    "y_pred_mlp = mlp_best_model.predict(X_test_scaled)\n",
    "\n",
    "# RMSE for MLP\n",
    "rmse_mlp = np.sqrt(mean_squared_error(y_test, y_pred_mlp))\n",
    "r2_mlp = r2_score(y_test, y_pred_mlp)\n",
    "\n",
    "print(f\"MLP Regressor - RMSE: {rmse_mlp:.4f}, R²: {r2_mlp:.4f}\")\n",
    "\n",
    "# Evaluating SGD Regressor\n",
    "sgd_best_model = sgd_random_search.best_estimator_\n",
    "y_pred_sgd = sgd_best_model.predict(X_test_scaled)\n",
    "\n",
    "# RMSE for SGD Regressor\n",
    "rmse_sgd = np.sqrt(mean_squared_error(y_test, y_pred_sgd))\n",
    "r2_sgd = r2_score(y_test, y_pred_sgd)\n",
    "\n",
    "print(f\"SGD Regressor - RMSE: {rmse_sgd:.4f}, R²: {r2_sgd:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff88ff8",
   "metadata": {},
   "source": [
    "# Stacking Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce0fac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StackingRegressor(estimators=[('rf',\n",
      "                               RandomForestRegressor(max_depth=10,\n",
      "                                                     min_samples_split=10,\n",
      "                                                     n_estimators=300,\n",
      "                                                     random_state=42)),\n",
      "                              ('dt',\n",
      "                               DecisionTreeRegressor(max_depth=20,\n",
      "                                                     max_features='log2',\n",
      "                                                     min_samples_leaf=2,\n",
      "                                                     min_samples_split=10,\n",
      "                                                     random_state=42)),\n",
      "                              ('svr', SVR(C=100, kernel='linear')),\n",
      "                              ('mlp',\n",
      "                               MLPRegressor(activation='tanh',\n",
      "                                            hidden_layer_sizes=(150,),\n",
      "                                            learning_rate='adaptive',\n",
      "                                            random_state=42, solver='sgd')),\n",
      "                              ('sgd',\n",
      "                               SGDRegressor(alpha=0.001, max_iter=1500,\n",
      "                                            random_state=42))],\n",
      "                  final_estimator=LinearRegression())\n",
      "<class 'sklearn.ensemble._stacking.StackingRegressor'>\n",
      "Stacking Model - RMSE: 3511778.5624, R²: -0.0081\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Base models (already tuned)\n",
    "base_learners = [\n",
    "    ('rf', rf_best_model),  # Random Forest\n",
    "    ('dt', dt_best_model),  # Decision Tree\n",
    "    ('svr', svr_best_model),  # Support Vector Regressor\n",
    "    ('mlp', mlp_best_model),  # MLP Regressor\n",
    "    ('sgd', sgd_best_model)  # SGD Regressor\n",
    "]\n",
    "\n",
    "# Meta-model (Linear Regression)\n",
    "meta_model = LinearRegression()\n",
    "\n",
    "# Stacking Regressor\n",
    "stacking_model = StackingRegressor(estimators=base_learners, final_estimator=meta_model)\n",
    "\n",
    "# Train the stacking model\n",
    "stacking_model.fit(X_train_scaled, y_train)\n",
    "print(stacking_model)\n",
    "# Predictions using the stacking model\n",
    "y_pred_stack = stacking_model.predict(X_test_scaled)\n",
    "print(type(stacking_model))\n",
    "\n",
    "# Evaluate the stacking model\n",
    "rmse_stack = np.sqrt(mean_squared_error(y_test, y_pred_stack))\n",
    "r2_stack = r2_score(y_test, y_pred_stack)\n",
    "\n",
    "print(f\"Stacking Model - RMSE: {rmse_stack:.4f}, R²: {r2_stack:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aea226f",
   "metadata": {},
   "source": [
    "# HyperParameter Tuning of Stacking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e0fe85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Stacking Model parameters: {'final_estimator__alpha': 0.1}\n",
      "StackingRegressor(estimators=[('rf',\n",
      "                               RandomForestRegressor(max_depth=10,\n",
      "                                                     min_samples_split=10,\n",
      "                                                     n_estimators=300,\n",
      "                                                     random_state=42)),\n",
      "                              ('dt',\n",
      "                               DecisionTreeRegressor(max_depth=20,\n",
      "                                                     max_features='log2',\n",
      "                                                     min_samples_leaf=2,\n",
      "                                                     min_samples_split=10,\n",
      "                                                     random_state=42)),\n",
      "                              ('svr', SVR(C=100, kernel='linear')),\n",
      "                              ('mlp',\n",
      "                               MLPRegressor(activation='tanh',\n",
      "                                            hidden_layer_sizes=(150,),\n",
      "                                            learning_rate='adaptive',\n",
      "                                            random_state=42, solver='sgd')),\n",
      "                              ('sgd',\n",
      "                               SGDRegressor(alpha=0.001, max_iter=1500,\n",
      "                                            random_state=42))],\n",
      "                  final_estimator=Ridge())\n",
      "['__abstractmethods__', '__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__sklearn_clone__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_build_request_for_signature', '_check_feature_names', '_check_n_features', '_clone_final_estimator', '_concatenate_predictions', '_doc_link_module', '_doc_link_template', '_doc_link_url_param_generator', '_estimator_type', '_get_default_requests', '_get_doc_link', '_get_metadata_request', '_get_param_names', '_get_params', '_get_tags', '_method_name', '_more_tags', '_parameter_constraints', '_replace_estimator', '_repr_html_', '_repr_html_inner', '_repr_mimebundle_', '_required_parameters', '_set_params', '_sk_visual_block_', '_sk_visual_block_with_final_estimator', '_sklearn_auto_wrap_output_keys', '_transform', '_validate_data', '_validate_estimators', '_validate_final_estimator', '_validate_names', '_validate_params', 'cv', 'estimators', 'final_estimator', 'fit', 'fit_transform', 'get_feature_names_out', 'get_metadata_routing', 'get_params', 'n_features_in_', 'n_jobs', 'named_estimators', 'passthrough', 'predict', 'score', 'set_fit_request', 'set_output', 'set_params', 'set_score_request', 'stack_method', 'transform', 'verbose']\n",
      "Stacking Model - RMSE: 3511778.5624, R²: -0.0081\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameter grid for the meta-model (Ridge in this case)\n",
    "param_grid_meta = {\n",
    "    'final_estimator__alpha': [0.1, 1, 10, 100, 1000],  # for Ridge (meta-model)\n",
    "}\n",
    "\n",
    "# Stack the best models with the meta-model (Ridge)\n",
    "stacking_model = StackingRegressor(estimators=base_learners, final_estimator=Ridge())\n",
    "\n",
    "# Use RandomizedSearchCV for the stacking model\n",
    "stacking_search = RandomizedSearchCV(stacking_model, param_grid_meta, n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
    "stacking_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best Stacking Model\n",
    "best_stacking_model = stacking_search.best_estimator_\n",
    "print(f\"Best Stacking Model parameters: {stacking_search.best_params_}\")\n",
    "print(stacking_model)\n",
    "print(dir(stacking_model))  # This will show all the available attributes and methods\n",
    "\n",
    "# Predictions using the best stacked model\n",
    "y_pred_stack = best_stacking_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the best stacking model\n",
    "rmse_stack = np.sqrt(mean_squared_error(y_test, y_pred_stack))\n",
    "r2_stack = r2_score(y_test, y_pred_stack)\n",
    "\n",
    "print(f\"Stacking Model - RMSE: {rmse_stack:.4f}, R²: {r2_stack:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f06e47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['City', 'City Group', 'Type', 'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7',\n",
      "       'P8', 'P9', 'P10', 'P11', 'P12', 'P13', 'P14', 'P15', 'P16', 'P17',\n",
      "       'P18', 'P19', 'P20', 'P21', 'P22', 'P23', 'P24', 'P25', 'P26', 'P27',\n",
      "       'P28', 'P29', 'P30', 'P31', 'P32', 'P33', 'P34', 'P35', 'P36', 'P37'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check the column names of the test DataFrame\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862c686d",
   "metadata": {},
   "source": [
    "# Generating Predictions and Submission files of Individual and Stacked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e315a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Decision Tree submission to: C:\\Users\\ayush\\Downloads\\decision_tree_submission.csv\n",
      "Saved Random Forest submission to: C:\\Users\\ayush\\Downloads\\random_forest_submission.csv\n",
      "Saved SVR submission to: C:\\Users\\ayush\\Downloads\\svr_submission.csv\n",
      "Saved MLP submission to: C:\\Users\\ayush\\Downloads\\mlp_submission.csv\n",
      "Saved SGD submission to: C:\\Users\\ayush\\Downloads\\sgd_submission.csv\n",
      "Saved Stacked Model submission to: C:\\Users\\ayush\\Downloads\\stacked_model_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Add a new column 'ID' based on row index\n",
    "test['ID'] = test.index\n",
    "\n",
    "# Create the base submission DataFrame with the new 'ID' column\n",
    "submission_base = pd.DataFrame({'ID': test['ID']})\n",
    "\n",
    "# Dictionary to store predictions from individual models\n",
    "individual_model_predictions = {\n",
    "    \"Decision Tree\": dt_best_model.predict(X_test_selected),\n",
    "    \"Random Forest\": rf_best_model.predict(X_test_selected),\n",
    "    \"SVR\": svr_best_model.predict(X_test_selected),\n",
    "    \"MLP\": mlp_best_model.predict(X_test_selected),\n",
    "    \"SGD\": sgd_best_model.predict(X_test_selected),\n",
    "}\n",
    "\n",
    "# Save predictions for each individual model\n",
    "for model_name, predictions in individual_model_predictions.items():\n",
    "    submission = submission_base.copy()\n",
    "    submission['Prediction'] = predictions\n",
    "    filename = f'C:\\\\Users\\\\ayush\\\\Downloads\\\\{model_name.replace(\" \", \"_\").lower()}_submission.csv'\n",
    "    submission.to_csv(filename, index=False)\n",
    "    print(f\"Saved {model_name} submission to: {filename}\")\n",
    "\n",
    "# Stacked model predictions\n",
    "stacked_predictions = best_stacking_model.predict(X_test_selected)  # Stacked model predictions\n",
    "stacked_submission = submission_base.copy()\n",
    "stacked_submission['Prediction'] = stacked_predictions\n",
    "stacked_filename = r'C:\\Users\\ayush\\Downloads\\stacked_model_submission.csv'\n",
    "stacked_submission.to_csv(stacked_filename, index=False)\n",
    "print(f\"Saved Stacked Model submission to: {stacked_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79f03614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137, 41)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e83ee84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 41)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d78be84",
   "metadata": {},
   "source": [
    "# Feature Selection using SequentialFeatureSelector and redoing Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc8b2cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "  RMSE: 3392329.7664\n",
      "  R2: 0.0526\n",
      "Decision Tree:\n",
      "  RMSE: 3682037.7844\n",
      "  R2: -0.1161\n",
      "MLP:\n",
      "  RMSE: 6139118.1284\n",
      "  R2: -2.1026\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assuming X_train and y_train are already defined\n",
    "# Let's scale the features before feature selection\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'Decision Tree': DecisionTreeRegressor(),\n",
    "    'MLP': MLPRegressor(max_iter=1000)\n",
    "}\n",
    "\n",
    "# Initialize the number of features to select\n",
    "n_features_to_select = 9  # You can change this number\n",
    "\n",
    "# Function to calculate RMSE and R2 for each model\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, n_features_to_select):\n",
    "    # Perform sequential feature selection\n",
    "    selector = SequentialFeatureSelector(model, n_features_to_select=n_features_to_select, direction='forward', cv=5)\n",
    "    selector.fit(X_train, y_train)\n",
    "\n",
    "    # Get the selected features\n",
    "    selected_features = selector.get_support()\n",
    "\n",
    "    # Train model with selected features\n",
    "    X_train_selected = X_train[:, selected_features]\n",
    "    X_test_selected = X_test[:, selected_features]\n",
    "\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_selected)\n",
    "    \n",
    "    # Calculate RMSE and R2\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    return rmse, r2\n",
    "\n",
    "# Split data into train/test (or use your predefined data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Dictionary to store performance metrics\n",
    "performance = {}\n",
    "\n",
    "# Loop through models and evaluate\n",
    "for model_name, model in models.items():\n",
    "    rmse, r2 = evaluate_model(model, X_train, y_train, X_test, y_test, n_features_to_select)\n",
    "    performance[model_name] = {'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "# Print performance metrics\n",
    "for model_name, metrics in performance.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  RMSE: {metrics['RMSE']:.4f}\")\n",
    "    print(f\"  R2: {metrics['R2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79388216",
   "metadata": {},
   "source": [
    "# Feature Selection Using GeneticSelectionCV using deap Module and redoing Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5d9738a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\n",
      "0  \t50    \n",
      "1  \t37    \n",
      "2  \t35    \n",
      "3  \t38    \n",
      "4  \t44    \n",
      "5  \t31    \n",
      "6  \t37    \n",
      "7  \t41    \n",
      "8  \t40    \n",
      "9  \t44    \n",
      "10 \t34    \n",
      "11 \t45    \n",
      "12 \t41    \n",
      "13 \t48    \n",
      "14 \t36    \n",
      "15 \t40    \n",
      "16 \t36    \n",
      "17 \t36    \n",
      "18 \t41    \n",
      "19 \t39    \n",
      "20 \t36    \n",
      "21 \t37    \n",
      "22 \t37    \n",
      "23 \t40    \n",
      "24 \t39    \n",
      "25 \t40    \n",
      "26 \t37    \n",
      "27 \t39    \n",
      "28 \t35    \n",
      "29 \t39    \n",
      "30 \t34    \n",
      "31 \t36    \n",
      "32 \t37    \n",
      "33 \t37    \n",
      "34 \t38    \n",
      "35 \t33    \n",
      "36 \t38    \n",
      "37 \t36    \n",
      "38 \t42    \n",
      "39 \t37    \n",
      "40 \t41    \n",
      "Selected features for Decision Tree: [0, 1, 2, 3, 5, 6, 8, 9]\n",
      "RMSE for Decision Tree with selected features: 1985643.9273\n",
      "gen\tnevals\n",
      "0  \t50    \n",
      "1  \t36    \n",
      "2  \t38    \n",
      "3  \t40    \n",
      "4  \t34    \n",
      "5  \t34    \n",
      "6  \t40    \n",
      "7  \t42    \n",
      "8  \t40    \n",
      "9  \t34    \n",
      "10 \t37    \n",
      "11 \t31    \n",
      "12 \t35    \n",
      "13 \t43    \n",
      "14 \t40    \n",
      "15 \t42    \n",
      "16 \t38    \n",
      "17 \t36    \n",
      "18 \t38    \n",
      "19 \t37    \n",
      "20 \t41    \n",
      "21 \t33    \n",
      "22 \t30    \n",
      "23 \t41    \n",
      "24 \t44    \n",
      "25 \t39    \n",
      "26 \t41    \n",
      "27 \t39    \n",
      "28 \t35    \n",
      "29 \t30    \n",
      "30 \t34    \n",
      "31 \t31    \n",
      "32 \t44    \n",
      "33 \t38    \n",
      "34 \t42    \n",
      "35 \t44    \n",
      "36 \t45    \n",
      "37 \t38    \n",
      "38 \t34    \n",
      "39 \t37    \n",
      "40 \t42    \n",
      "Selected features for Random Forest: [0, 1, 3, 6, 9]\n",
      "RMSE for Random Forest with selected features: 1722095.7773\n",
      "gen\tnevals\n",
      "0  \t50    \n",
      "1  \t32    \n",
      "2  \t41    \n",
      "3  \t44    \n",
      "4  \t36    \n",
      "5  \t38    \n",
      "6  \t38    \n",
      "7  \t47    \n",
      "8  \t37    \n",
      "9  \t39    \n",
      "10 \t40    \n",
      "11 \t37    \n",
      "12 \t34    \n",
      "13 \t38    \n",
      "14 \t41    \n",
      "15 \t44    \n",
      "16 \t32    \n",
      "17 \t39    \n",
      "18 \t33    \n",
      "19 \t36    \n",
      "20 \t37    \n",
      "21 \t38    \n",
      "22 \t33    \n",
      "23 \t37    \n",
      "24 \t42    \n",
      "25 \t33    \n",
      "26 \t29    \n",
      "27 \t37    \n",
      "28 \t41    \n",
      "29 \t32    \n",
      "30 \t40    \n",
      "31 \t33    \n",
      "32 \t33    \n",
      "33 \t43    \n",
      "34 \t40    \n",
      "35 \t35    \n",
      "36 \t33    \n",
      "37 \t39    \n",
      "38 \t34    \n",
      "39 \t37    \n",
      "40 \t36    \n",
      "Selected features for MLP: [9]\n",
      "RMSE for MLP with selected features: 5046329.3120\n",
      "Decision Tree:\n",
      "  RMSE: 1985643.9273\n",
      "Random Forest:\n",
      "  RMSE: 1722095.7773\n",
      "MLP:\n",
      "  RMSE: 5046329.3120\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from deap import base, creator, tools, algorithms\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume X_train and y_train are predefined\n",
    "\n",
    "# Initialize creator to maximize fitness (higher fitness is better)\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# Define evaluation function (e.g., use RMSE as the objective to minimize)\n",
    "def evaluate(individual, model, X_train, y_train):\n",
    "    selected_features = [index for index, bit in enumerate(individual) if bit == 1]\n",
    "    if len(selected_features) == 0:  # Avoid empty feature set\n",
    "        return 0,\n",
    "    \n",
    "    # Select the features from the dataset\n",
    "    X_selected = X_train[:, selected_features]\n",
    "    \n",
    "    # Calculate RMSE using cross-validation\n",
    "    rmse = -cross_val_score(model, X_selected, y_train, cv=5, scoring=\"neg_root_mean_squared_error\").mean()\n",
    "    \n",
    "    return rmse,\n",
    "\n",
    "# Create a population of individuals (binary feature selection)\n",
    "def create_individual():\n",
    "    return [random.randint(0, 1) for _ in range(X_train.shape[1])]\n",
    "\n",
    "# Initialize models for evaluation\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeRegressor(),\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'MLP': MLPRegressor(max_iter=1000)\n",
    "}\n",
    "\n",
    "# Initialize the population and set up the evolutionary algorithm\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, create_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.1)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# Run the genetic algorithm\n",
    "def run_genetic_algorithm(model, X_train, y_train):\n",
    "    population = toolbox.population(n=50)\n",
    "    \n",
    "    # Define the evaluation function in the DEAP framework\n",
    "    toolbox.register(\"evaluate\", evaluate, model=model, X_train=X_train, y_train=y_train)\n",
    "    \n",
    "    # Run the evolutionary algorithm\n",
    "    algorithms.eaSimple(population, toolbox, cxpb=0.7, mutpb=0.2, ngen=40, verbose=True)\n",
    "    \n",
    "    return population\n",
    "\n",
    "# Split data into train/test (you should have X_train and y_train already defined)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Loop through models, run the genetic algorithm and evaluate performance\n",
    "performance = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Run the genetic algorithm for feature selection\n",
    "    population = run_genetic_algorithm(model, X_train, y_train)\n",
    "    \n",
    "    # Extract the best individual (best feature set)\n",
    "    best_individual = tools.selBest(population, 1)[0]\n",
    "    selected_features = [index for index, bit in enumerate(best_individual) if bit == 1]\n",
    "    print(f\"Selected features for {model_name}: {selected_features}\")\n",
    "    \n",
    "    # Train the model using the selected features\n",
    "    X_selected = X_train[:, selected_features]\n",
    "    model.fit(X_selected, y_train)\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    X_test_selected = X_test[:, selected_features]\n",
    "    y_pred = model.predict(X_test_selected)\n",
    "    \n",
    "    # Calculate RMSE for the selected features\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    performance[model_name] = {'RMSE': rmse}\n",
    "    print(f\"RMSE for {model_name} with selected features: {rmse:.4f}\")\n",
    "\n",
    "# Print performance metrics for all models\n",
    "for model_name, metrics in performance.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  RMSE: {metrics['RMSE']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baa15db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn-genetic\n",
      "  Using cached sklearn_genetic-0.6.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from sklearn-genetic) (1.1.2)\n",
      "Requirement already satisfied: deap>=1.0.2 in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from sklearn-genetic) (1.4.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from sklearn-genetic) (1.20.3)\n",
      "Collecting multiprocess (from sklearn-genetic)\n",
      "  Using cached multiprocess-0.70.17-py39-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0->sklearn-genetic) (1.7.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0->sklearn-genetic) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0->sklearn-genetic) (3.5.0)\n",
      "Collecting dill>=0.3.9 (from multiprocess->sklearn-genetic)\n",
      "  Using cached dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Using cached sklearn_genetic-0.6.0-py3-none-any.whl (11 kB)\n",
      "Using cached multiprocess-0.70.17-py39-none-any.whl (133 kB)\n",
      "Using cached dill-0.3.9-py3-none-any.whl (119 kB)\n",
      "Installing collected packages: dill, multiprocess, sklearn-genetic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ayush\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Error parsing dependencies of pyodbc: Invalid version: '4.0.0-unsupported'\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ayush\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 105, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "  File \"C:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 96, in _inner_run\n",
      "    return self.run(options, args)\n",
      "  File \"C:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 67, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 483, in run\n",
      "    installed_versions[distribution.canonical_name] = distribution.version\n",
      "  File \"C:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pip\\_internal\\metadata\\pkg_resources.py\", line 192, in version\n",
      "    return parse_version(self._dist.version)\n",
      "  File \"C:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pip\\_vendor\\packaging\\version.py\", line 56, in parse\n",
      "    return Version(version)\n",
      "  File \"C:\\Users\\ayush\\anaconda3\\lib\\site-packages\\pip\\_vendor\\packaging\\version.py\", line 202, in __init__\n",
      "    raise InvalidVersion(f\"Invalid version: '{version}'\")\n",
      "pip._vendor.packaging.version.InvalidVersion: Invalid version: '4.0.0-unsupported'\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn-genetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0b2eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genetic_selection import GeneticSelectionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f566e",
   "metadata": {},
   "source": [
    "# Feature Selection Using GeneticSelectionCV and redoing Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93dd1117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "  RMSE: 1752541.8435\n",
      "  R2: -0.2254\n",
      "  Selected Features: [0 4 7 8]\n",
      "Decision Tree:\n",
      "  RMSE: 1565031.2270\n",
      "  R2: 0.0228\n",
      "  Selected Features: [0 2 3 4 6 7 8]\n",
      "MLP:\n",
      "  RMSE: 4900108.9652\n",
      "  R2: -8.5798\n",
      "  Selected Features: [0 1 2 3 4 5 6 7 9]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'Decision Tree': DecisionTreeRegressor(),\n",
    "    'MLP': MLPRegressor(max_iter=1000)\n",
    "}\n",
    "\n",
    "# Function to calculate RMSE and R2 for each model\n",
    "def evaluate_model_with_genetic_selection(model, X_train, y_train, X_test, y_test):\n",
    "    # Perform genetic feature selection\n",
    "    selector = GeneticSelectionCV(estimator=model,\n",
    "                                  cv=5,  # Number of cross-validation folds\n",
    "                                  verbose=0,  # Set to 1 or 2 to get verbose output\n",
    "                                  scoring='neg_root_mean_squared_error',  # Use RMSE as the scoring metric\n",
    "                                  n_population=20,  # Number of individuals in the population\n",
    "                                  crossover_proba=0.5,  # Probability of crossover\n",
    "                                  n_generations=20,  # Number of generations\n",
    "                                  n_jobs=-1)  # Use all cores available\n",
    "    \n",
    "    # Fit the selector to the data\n",
    "    selector = selector.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the selected features\n",
    "    selected_features = selector.support_\n",
    "\n",
    "    # Check if any features were selected\n",
    "    if np.sum(selected_features) == 0:\n",
    "        print(f\"Warning: No features selected for model {model}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Train model with selected features\n",
    "    X_train_selected = X_train[:, selected_features]\n",
    "    X_test_selected = X_test[:, selected_features]\n",
    "\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test_selected)\n",
    "    \n",
    "    # Calculate RMSE and R2\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    return rmse, r2, selected_features\n",
    "\n",
    "# Split data into train/test (or use your predefined data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Dictionary to store performance metrics\n",
    "performance = {}\n",
    "\n",
    "# Loop through models and evaluate\n",
    "for model_name, model in models.items():\n",
    "    rmse, r2, selected_features = evaluate_model_with_genetic_selection(model, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    if rmse is not None and r2 is not None:\n",
    "        performance[model_name] = {'RMSE': rmse, 'R2': r2, 'Selected Features': selected_features}\n",
    "    else:\n",
    "        print(f\"Skipping model {model_name} due to feature selection failure\")\n",
    "\n",
    "# Print performance metrics\n",
    "for model_name, metrics in performance.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  RMSE: {metrics['RMSE']:.4f}\")\n",
    "    print(f\"  R2: {metrics['R2']:.4f}\")\n",
    "    print(f\"  Selected Features: {np.where(metrics['Selected Features'])[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
